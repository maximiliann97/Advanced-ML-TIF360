{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\OneDrive\\Dokument\\Chalmers\\Advanced machine learning with neural networks\\hwc_venv\\Lib\\site-packages\\deeptrack\\backend\\_config.py:11: UserWarning: cupy not installed. GPU-accelerated simulations will not be possible\n",
      "  warnings.warn(\n",
      "C:\\Users\\maxim\\OneDrive\\Dokument\\Chalmers\\Advanced machine learning with neural networks\\hwc_venv\\Lib\\site-packages\\deeptrack\\backend\\_config.py:25: UserWarning: cupy not installed, CPU acceleration not enabled\n",
      "  warnings.warn(\"cupy not installed, CPU acceleration not enabled\")\n",
      "C:\\Users\\maxim\\OneDrive\\Dokument\\Chalmers\\Advanced machine learning with neural networks\\hwc_venv\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\maxim\\OneDrive\\Dokument\\Chalmers\\Advanced machine learning with neural networks\\hwc_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import deeptrack as dt\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "sequence_length = 10  # Number of frames per sequence\n",
    "MIN_SIZE = 0.5e-6\n",
    "MAX_SIZE = 1.5e-6\n",
    "MAX_VEL = 10  # Maximum velocity. The higher the trickier!\n",
    "MAX_PARTICLES = 3  # Max number of particles in each sequence. The higher the trickier!\n",
    "\n",
    "# Defining properties of the particles\n",
    "particle = dt.Sphere(\n",
    "    intensity=lambda: 10 + 10 * np.random.rand(),\n",
    "    radius=lambda: MIN_SIZE + np.random.rand() * (MAX_SIZE - MIN_SIZE),\n",
    "    position=lambda: IMAGE_SIZE * np.random.rand(2),\n",
    "    vel=lambda: MAX_VEL * np.random.rand(2),\n",
    "    position_unit=\"pixel\",\n",
    ")\n",
    "\n",
    "# Defining an update rule for the particle position\n",
    "def get_position(previous_value, vel):\n",
    "\n",
    "    newv = previous_value + vel\n",
    "    for i in range(2):\n",
    "        if newv[i] > 63:\n",
    "            newv[i] = 63 - np.abs(newv[i] - 63)\n",
    "            vel[i] = -vel[i]\n",
    "        elif newv[i] < 0:\n",
    "            newv[i] = np.abs(newv[i])\n",
    "            vel[i] = -vel[i]\n",
    "    return newv\n",
    "\n",
    "\n",
    "particle = dt.Sequential(particle, position=get_position)\n",
    "\n",
    "# Defining properties of the microscope\n",
    "optics = dt.Fluorescence(\n",
    "    NA=1,\n",
    "    output_region=(0, 0, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    magnification=10,\n",
    "    resolution=(1e-6, 1e-6, 1e-6),\n",
    "    wavelength=633e-9,\n",
    ")\n",
    "\n",
    "# Combining everything into a dataset.\n",
    "# Note that the sequences are flipped in different directions, so that each unique sequence defines\n",
    "# in fact 8 sequences flipped in different directions, to speed up data generation\n",
    "sequential_images = dt.Sequence(\n",
    "    optics(particle ** (lambda: 1 + np.random.randint(MAX_PARTICLES))),\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "dataset = sequential_images >> dt.FlipUD() >> dt.FlipDiagonal() >> dt.FlipLR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, layers, losses\n",
    "import tensorflow as tf\n",
    "Layer=keras.layers.Layer\n",
    "Conv2D=keras.layers.Conv2D\n",
    "MaxPool2D=keras.layers.MaxPooling2D\n",
    "Dense=keras.layers.Dense\n",
    "Flatten=keras.layers.Flatten\n",
    "Reshape=keras.layers.Reshape\n",
    "\n",
    "class Time2Vector(Layer): #Time embedding layer\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.math.reduce_mean(x[:,:,:], axis=-1) # Convert (batch, seq_len, 5) to (batch, seq_len)\n",
    "        time_linear = self.weights_linear * x + self.bias_linear\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # (batch, seq_len, 1)\n",
    "\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # (batch, seq_len, 1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # (batch, seq_len, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(Layer): #Attention layer\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "        self.key = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "        self.value = Dense(self.d_v, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, h, d_f):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.heads = h\n",
    "        self.d_f = d_f\n",
    "        self.attn_layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        for head in range(self.heads):\n",
    "            self.attn_layers.append(SingleAttention(self.d_k, self.d_v))\n",
    "        self.dense = Dense(self.d_f, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, input):\n",
    "        attention = [self.attn_layers[i](input) for i in range(self.heads)]\n",
    "        conc_attention = tf.concat(attention, axis=1)\n",
    "        mlp = self.linear(conc_attention)\n",
    "        return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, h, d_f, d_filt):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.heads = h\n",
    "        self.d_f = d_f\n",
    "        self.d_filt = d_filt\n",
    "    def build(self, input_shape):\n",
    "        self.multi_head = MultiAttention(self.d_k, self.d_v, self.h, self_d_f)\n",
    "        self.dropout = Dropout(rate=0.2)\n",
    "        self.norm1 = LayerNormalization(epsilon=0.0001)\n",
    "        self.conv1 = Conv2D(filters=self.d_f, kernel_size=1, activation='relu')\n",
    "        self.conv2 = Conv2D(filters=self.d_filt, kernel_size=1)\n",
    "        self.norm2 = LayerNormalization(epsilon=0.0001)\n",
    "        \n",
    "    \n",
    "    def call(self, input):\n",
    "        res = input[0]\n",
    "        \n",
    "        #Attention\n",
    "        x = self.multi_head(input)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm1(x + res)\n",
    "        \n",
    "        #Feed-forward\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x + res)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [14:04<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "def retrieve_data(data_size):\n",
    "    frames = []\n",
    "\n",
    "    for d in trange(data_size):\n",
    "        video = dataset.update().resolve()\n",
    "        for frame in video:\n",
    "            frames.append(frame)\n",
    "\n",
    "    return tf.stack(frames)\n",
    "\n",
    "data_size = 1000\n",
    "\n",
    "# Save the data\n",
    "data = retrieve_data(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('video_data.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load if data is already generated\n",
    "data = np.load('video_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (8000, 64, 64, 1)\n",
      "Validation data size: (2000, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "num_frames = len(data)\n",
    "train_size = int(num_frames * 0.8)\n",
    "val_size = num_frames - train_size\n",
    "\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "\n",
    "max_val = tf.reduce_max(tf.concat([train_data, val_data], axis=0))\n",
    "train_data /= max_val\n",
    "val_data /= max_val\n",
    "\n",
    "print(\"Training data size:\", train_data.shape)\n",
    "print(\"Validation data size:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 64)        1088      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 16)          16400     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,488\n",
      "Trainable params: 17,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 4, 4, 16)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 16, 16, 16)       4112      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 64, 64, 64)       65600     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 64, 64, 1)         65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,777\n",
      "Trainable params: 69,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Conv2D=keras.layers.Conv2D\n",
    "MaxPool2D=keras.layers.MaxPooling2D\n",
    "Dense=keras.layers.Dense\n",
    "Flatten=keras.layers.Flatten\n",
    "Reshape=keras.layers.Reshape\n",
    "Input = keras.layers.Input\n",
    "Sequential = keras.Sequential\n",
    "Conv2DTranspose = keras.layers.Conv2DTranspose\n",
    "\n",
    "k_size = 4\n",
    "n_filters = 16\n",
    "bottleneck_size = k_size**2*n_filters\n",
    "\n",
    "class AutoEncoder(Model):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Sequential([\n",
    "            Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1)),\n",
    "            Conv2D(64, (4, 4), activation='relu', padding='same', strides=4),\n",
    "            Conv2D(n_filters, (k_size, k_size), activation='relu', padding='same', strides=4),\n",
    "            Flatten()\n",
    "            ])\n",
    "        \n",
    "        bottleneck_size = k_size**2*n_filters\n",
    "        self.decoder = Sequential([\n",
    "            Input(shape=(bottleneck_size,)),\n",
    "            Reshape(target_shape=(4, 4, n_filters)),\n",
    "            Conv2DTranspose(n_filters, (4, 4), strides=(4, 4), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(64, (8, 8), strides=(4, 4), activation='relu', padding='same'),\n",
    "            Conv2D(1, (1, 1), activation='linear', padding='same')\n",
    "            ])\n",
    "    def call(self, input):\n",
    "        x = self.encoder(input)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder.encoder.build(input_shape=(None, 64, 64, 1))\n",
    "autoencoder.encoder.summary()\n",
    "\n",
    "autoencoder.decoder.build(input_shape=(None, bottleneck_size))\n",
    "autoencoder.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "250/250 [==============================] - 42s 162ms/step - loss: 0.0012 - val_loss: 1.8534e-04\n",
      "Epoch 2/40\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 1.3473e-04 - val_loss: 8.8952e-05\n",
      "Epoch 3/40\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 7.3637e-05 - val_loss: 5.5029e-05\n",
      "Epoch 4/40\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 4.8939e-05 - val_loss: 4.3830e-05\n",
      "Epoch 5/40\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 3.7622e-05 - val_loss: 3.0508e-05\n",
      "Epoch 6/40\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 2.9372e-05 - val_loss: 2.4288e-05\n",
      "Epoch 7/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 2.4844e-05 - val_loss: 1.9783e-05\n",
      "Epoch 8/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 2.0574e-05 - val_loss: 1.8649e-05\n",
      "Epoch 9/40\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 1.7880e-05 - val_loss: 1.4831e-05\n",
      "Epoch 10/40\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 1.6118e-05 - val_loss: 1.6991e-05\n",
      "Epoch 11/40\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 1.4727e-05 - val_loss: 1.2556e-05\n",
      "Epoch 12/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 2.0047e-05 - val_loss: 1.0828e-05\n",
      "Epoch 13/40\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 1.1208e-05 - val_loss: 9.7398e-06\n",
      "Epoch 14/40\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 1.1639e-05 - val_loss: 1.2522e-05\n",
      "Epoch 15/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 1.1574e-05 - val_loss: 1.1334e-05\n",
      "Epoch 16/40\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 1.2956e-05 - val_loss: 7.7234e-06\n",
      "Epoch 17/40\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 1.0036e-05 - val_loss: 7.6790e-06\n",
      "Epoch 18/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 1.2944e-05 - val_loss: 7.3093e-06\n",
      "Epoch 19/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 9.3167e-06 - val_loss: 7.3126e-06\n",
      "Epoch 20/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 7.5796e-06 - val_loss: 6.7283e-06\n",
      "Epoch 21/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 1.9866e-05 - val_loss: 6.2311e-06\n",
      "Epoch 22/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 6.4197e-06 - val_loss: 5.9304e-06\n",
      "Epoch 23/40\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 6.3404e-06 - val_loss: 5.5156e-06\n",
      "Epoch 24/40\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 6.3733e-06 - val_loss: 5.8217e-06\n",
      "Epoch 25/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 1.6675e-05 - val_loss: 9.0966e-06\n",
      "Epoch 26/40\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 6.1503e-06 - val_loss: 5.0823e-06\n",
      "Epoch 27/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 5.1705e-06 - val_loss: 4.7465e-06\n",
      "Epoch 28/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 5.4716e-06 - val_loss: 7.1606e-06\n",
      "Epoch 29/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 1.3205e-05 - val_loss: 5.2185e-06\n",
      "Epoch 30/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 4.9001e-06 - val_loss: 4.3371e-06\n",
      "Epoch 31/40\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 4.6037e-06 - val_loss: 4.4437e-06\n",
      "Epoch 32/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 1.3179e-05 - val_loss: 4.3185e-06\n",
      "Epoch 33/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 4.4355e-06 - val_loss: 4.7419e-06\n",
      "Epoch 34/40\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 4.3248e-06 - val_loss: 5.2252e-06\n",
      "Epoch 35/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 6.4030e-06 - val_loss: 5.9202e-06\n",
      "Epoch 36/40\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 4.3637e-06 - val_loss: 5.0339e-06\n",
      "Epoch 37/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 4.7853e-06 - val_loss: 1.5079e-05\n",
      "Epoch 38/40\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 2.4892e-05 - val_loss: 3.7557e-06\n",
      "Epoch 39/40\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 3.6906e-06 - val_loss: 3.7791e-06\n",
      "Epoch 40/40\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 3.5605e-06 - val_loss: 3.3310e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2729debdb50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(train_data, train_data,\n",
    "                epochs=40,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_data, val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: autoencoder_bottleneck_size=256\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: autoencoder_bottleneck_size=256\\assets\n"
     ]
    }
   ],
   "source": [
    "autoencoder.save('autoencoder_bottleneck_size='+str(bottleneck_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwc_venv",
   "language": "python",
   "name": "hwc_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
