{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\OneDrive\\Dokument\\Chalmers\\Advanced machine learning with neural networks\\hwc_venv\\Lib\\site-packages\\deeptrack\\backend\\_config.py:11: UserWarning: cupy not installed. GPU-accelerated simulations will not be possible\n",
      "  warnings.warn(\n",
      "C:\\Users\\maxim\\OneDrive\\Dokument\\Chalmers\\Advanced machine learning with neural networks\\hwc_venv\\Lib\\site-packages\\deeptrack\\backend\\_config.py:25: UserWarning: cupy not installed, CPU acceleration not enabled\n",
      "  warnings.warn(\"cupy not installed, CPU acceleration not enabled\")\n",
      "C:\\Users\\maxim\\OneDrive\\Dokument\\Chalmers\\Advanced machine learning with neural networks\\hwc_venv\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\maxim\\OneDrive\\Dokument\\Chalmers\\Advanced machine learning with neural networks\\hwc_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import deeptrack as dt\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "sequence_length = 10  # Number of frames per sequence\n",
    "MIN_SIZE = 0.5e-6\n",
    "MAX_SIZE = 1.5e-6\n",
    "MAX_VEL = 10  # Maximum velocity. The higher the trickier!\n",
    "MAX_PARTICLES = 3  # Max number of particles in each sequence. The higher the trickier!\n",
    "\n",
    "# Defining properties of the particles\n",
    "particle = dt.Sphere(\n",
    "    intensity=lambda: 10 + 10 * np.random.rand(),\n",
    "    radius=lambda: MIN_SIZE + np.random.rand() * (MAX_SIZE - MIN_SIZE),\n",
    "    position=lambda: IMAGE_SIZE * np.random.rand(2),\n",
    "    vel=lambda: MAX_VEL * np.random.rand(2),\n",
    "    position_unit=\"pixel\",\n",
    ")\n",
    "\n",
    "# Defining an update rule for the particle position\n",
    "def get_position(previous_value, vel):\n",
    "\n",
    "    newv = previous_value + vel\n",
    "    for i in range(2):\n",
    "        if newv[i] > 63:\n",
    "            newv[i] = 63 - np.abs(newv[i] - 63)\n",
    "            vel[i] = -vel[i]\n",
    "        elif newv[i] < 0:\n",
    "            newv[i] = np.abs(newv[i])\n",
    "            vel[i] = -vel[i]\n",
    "    return newv\n",
    "\n",
    "\n",
    "particle = dt.Sequential(particle, position=get_position)\n",
    "\n",
    "# Defining properties of the microscope\n",
    "optics = dt.Fluorescence(\n",
    "    NA=1,\n",
    "    output_region=(0, 0, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    magnification=10,\n",
    "    resolution=(1e-6, 1e-6, 1e-6),\n",
    "    wavelength=633e-9,\n",
    ")\n",
    "\n",
    "# Combining everything into a dataset.\n",
    "# Note that the sequences are flipped in different directions, so that each unique sequence defines\n",
    "# in fact 8 sequences flipped in different directions, to speed up data generation\n",
    "sequential_images = dt.Sequence(\n",
    "    optics(particle ** (lambda: 1 + np.random.randint(MAX_PARTICLES))),\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "dataset = sequential_images >> dt.FlipUD() >> dt.FlipDiagonal() >> dt.FlipLR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, layers, losses\n",
    "import tensorflow as tf\n",
    "Layer=keras.layers.Layer\n",
    "Conv2D=keras.layers.Conv2D\n",
    "MaxPool2D=keras.layers.MaxPooling2D\n",
    "Dense=keras.layers.Dense\n",
    "Flatten=keras.layers.Flatten\n",
    "Reshape=keras.layers.Reshape\n",
    "\n",
    "class Time2Vector(Layer): #Time embedding layer\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.math.reduce_mean(x[:,:,:], axis=-1) # Convert (batch, seq_len, 5) to (batch, seq_len)\n",
    "        time_linear = self.weights_linear * x + self.bias_linear\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # (batch, seq_len, 1)\n",
    "\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # (batch, seq_len, 1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # (batch, seq_len, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(Layer): #Attention layer\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "        self.key = Dense(self.d_k, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "        self.value = Dense(self.d_v, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, h, d_f):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.heads = h\n",
    "        self.d_f = d_f\n",
    "        self.attn_layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        for head in range(self.heads):\n",
    "            self.attn_layers.append(SingleAttention(self.d_k, self.d_v))\n",
    "        self.dense = Dense(self.d_f, input_shape=input_shape, kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, input):\n",
    "        attention = [self.attn_layers[i](input) for i in range(self.heads)]\n",
    "        conc_attention = tf.concat(attention, axis=1)\n",
    "        mlp = self.linear(conc_attention)\n",
    "        return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, h, d_f, d_filt):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.heads = h\n",
    "        self.d_f = d_f\n",
    "        self.d_filt = d_filt\n",
    "    def build(self, input_shape):\n",
    "        self.multi_head = MultiAttention(self.d_k, self.d_v, self.h, self_d_f)\n",
    "        self.dropout = Dropout(rate=0.2)\n",
    "        self.norm1 = LayerNormalization(epsilon=0.0001)\n",
    "        self.conv1 = Conv2D(filters=self.d_f, kernel_size=1, activation='relu')\n",
    "        self.conv2 = Conv2D(filters=self.d_filt, kernel_size=1)\n",
    "        self.norm2 = LayerNormalization(epsilon=0.0001)\n",
    "        \n",
    "    \n",
    "    def call(self, input):\n",
    "        res = input[0]\n",
    "        \n",
    "        #Attention\n",
    "        x = self.multi_head(input)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm1(x + res)\n",
    "        \n",
    "        #Feed-forward\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x + res)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [09:54<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0.00092624]\n",
      "   [0.00093991]\n",
      "   [0.00097084]\n",
      "   ...\n",
      "   [0.00117568]\n",
      "   [0.00122628]\n",
      "   [0.00127562]]\n",
      "\n",
      "  [[0.00095456]\n",
      "   [0.00096992]\n",
      "   [0.00100557]\n",
      "   ...\n",
      "   [0.00109817]\n",
      "   [0.00113969]\n",
      "   [0.0011775 ]]\n",
      "\n",
      "  [[0.00101884]\n",
      "   [0.00100347]\n",
      "   [0.00099622]\n",
      "   ...\n",
      "   [0.00100818]\n",
      "   [0.00101746]\n",
      "   [0.00101997]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00750577]\n",
      "   [0.00688855]\n",
      "   [0.00644731]\n",
      "   ...\n",
      "   [0.00202924]\n",
      "   [0.00205514]\n",
      "   [0.002087  ]]\n",
      "\n",
      "  [[0.00619241]\n",
      "   [0.00554471]\n",
      "   [0.00504339]\n",
      "   ...\n",
      "   [0.00208191]\n",
      "   [0.00210063]\n",
      "   [0.00212973]]\n",
      "\n",
      "  [[0.00538733]\n",
      "   [0.00473134]\n",
      "   [0.00417268]\n",
      "   ...\n",
      "   [0.00210815]\n",
      "   [0.00212434]\n",
      "   [0.00216598]]]\n",
      "\n",
      "\n",
      " [[[0.00065942]\n",
      "   [0.00051877]\n",
      "   [0.00040042]\n",
      "   ...\n",
      "   [0.00100599]\n",
      "   [0.00113985]\n",
      "   [0.00122271]]\n",
      "\n",
      "  [[0.00060565]\n",
      "   [0.00044148]\n",
      "   [0.00030326]\n",
      "   ...\n",
      "   [0.00100426]\n",
      "   [0.0011808 ]\n",
      "   [0.00126278]]\n",
      "\n",
      "  [[0.00059359]\n",
      "   [0.00042816]\n",
      "   [0.00028965]\n",
      "   ...\n",
      "   [0.00100426]\n",
      "   [0.0011893 ]\n",
      "   [0.00127117]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.05620532]\n",
      "   [0.05929454]\n",
      "   [0.0614381 ]\n",
      "   ...\n",
      "   [0.00257764]\n",
      "   [0.0028052 ]\n",
      "   [0.00291549]]\n",
      "\n",
      "  [[0.04272076]\n",
      "   [0.04441688]\n",
      "   [0.04535417]\n",
      "   ...\n",
      "   [0.00263901]\n",
      "   [0.00275714]\n",
      "   [0.00286616]]\n",
      "\n",
      "  [[0.03231157]\n",
      "   [0.03354655]\n",
      "   [0.03430148]\n",
      "   ...\n",
      "   [0.00259792]\n",
      "   [0.0026146 ]\n",
      "   [0.00271356]]]\n",
      "\n",
      "\n",
      " [[[0.00232806]\n",
      "   [0.00196432]\n",
      "   [0.00156802]\n",
      "   ...\n",
      "   [0.00108345]\n",
      "   [0.00114814]\n",
      "   [0.0012081 ]]\n",
      "\n",
      "  [[0.00211155]\n",
      "   [0.00185794]\n",
      "   [0.00157664]\n",
      "   ...\n",
      "   [0.00097624]\n",
      "   [0.00103037]\n",
      "   [0.00109763]]\n",
      "\n",
      "  [[0.00204549]\n",
      "   [0.00184664]\n",
      "   [0.00162629]\n",
      "   ...\n",
      "   [0.00086522]\n",
      "   [0.00090757]\n",
      "   [0.0009643 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.10964393]\n",
      "   [0.16490327]\n",
      "   [0.2652939 ]\n",
      "   ...\n",
      "   [0.00069998]\n",
      "   [0.00076591]\n",
      "   [0.00084017]]\n",
      "\n",
      "  [[0.09658996]\n",
      "   [0.1439748 ]\n",
      "   [0.2123547 ]\n",
      "   ...\n",
      "   [0.00076983]\n",
      "   [0.00081031]\n",
      "   [0.00087347]]\n",
      "\n",
      "  [[0.08179376]\n",
      "   [0.12017965]\n",
      "   [0.17215238]\n",
      "   ...\n",
      "   [0.0008885 ]\n",
      "   [0.00093806]\n",
      "   [0.00097966]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.00494423]\n",
      "   [0.00495336]\n",
      "   [0.00468713]\n",
      "   ...\n",
      "   [0.00206798]\n",
      "   [0.00201041]\n",
      "   [0.00188969]]\n",
      "\n",
      "  [[0.00448839]\n",
      "   [0.00454163]\n",
      "   [0.00431594]\n",
      "   ...\n",
      "   [0.00189861]\n",
      "   [0.00185176]\n",
      "   [0.00177267]]\n",
      "\n",
      "  [[0.00404711]\n",
      "   [0.00420621]\n",
      "   [0.00414434]\n",
      "   ...\n",
      "   [0.00179383]\n",
      "   [0.0017514 ]\n",
      "   [0.00168689]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.04614248]\n",
      "   [0.06264741]\n",
      "   [0.08368642]\n",
      "   ...\n",
      "   [0.00040728]\n",
      "   [0.00044125]\n",
      "   [0.00058257]]\n",
      "\n",
      "  [[0.04667592]\n",
      "   [0.06340072]\n",
      "   [0.08528203]\n",
      "   ...\n",
      "   [0.0002933 ]\n",
      "   [0.00033129]\n",
      "   [0.00050206]]\n",
      "\n",
      "  [[0.04671107]\n",
      "   [0.06344725]\n",
      "   [0.08539199]\n",
      "   ...\n",
      "   [0.00028583]\n",
      "   [0.00032425]\n",
      "   [0.00049779]]]\n",
      "\n",
      "\n",
      " [[[0.00334838]\n",
      "   [0.00359258]\n",
      "   [0.00365856]\n",
      "   ...\n",
      "   [0.00135804]\n",
      "   [0.00135733]\n",
      "   [0.00136143]]\n",
      "\n",
      "  [[0.00330602]\n",
      "   [0.00356629]\n",
      "   [0.00357366]\n",
      "   ...\n",
      "   [0.00116748]\n",
      "   [0.00111699]\n",
      "   [0.00110291]]\n",
      "\n",
      "  [[0.00326246]\n",
      "   [0.00347645]\n",
      "   [0.0034587 ]\n",
      "   ...\n",
      "   [0.00103839]\n",
      "   [0.00099359]\n",
      "   [0.00097926]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.03014638]\n",
      "   [0.03742221]\n",
      "   [0.0491945 ]\n",
      "   ...\n",
      "   [0.0009108 ]\n",
      "   [0.00083755]\n",
      "   [0.00081682]]\n",
      "\n",
      "  [[0.03073757]\n",
      "   [0.03687326]\n",
      "   [0.04654132]\n",
      "   ...\n",
      "   [0.00108235]\n",
      "   [0.00102706]\n",
      "   [0.00101171]]\n",
      "\n",
      "  [[0.0306074 ]\n",
      "   [0.03606736]\n",
      "   [0.04359406]\n",
      "   ...\n",
      "   [0.00119547]\n",
      "   [0.00113382]\n",
      "   [0.00112495]]]\n",
      "\n",
      "\n",
      " [[[0.00293906]\n",
      "   [0.00328956]\n",
      "   [0.00355594]\n",
      "   ...\n",
      "   [0.00116293]\n",
      "   [0.00125951]\n",
      "   [0.00132822]]\n",
      "\n",
      "  [[0.00321548]\n",
      "   [0.00345294]\n",
      "   [0.00364691]\n",
      "   ...\n",
      "   [0.00134799]\n",
      "   [0.00150911]\n",
      "   [0.00160655]]\n",
      "\n",
      "  [[0.00329315]\n",
      "   [0.00343256]\n",
      "   [0.00356813]\n",
      "   ...\n",
      "   [0.00153772]\n",
      "   [0.00170174]\n",
      "   [0.00179048]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.02057993]\n",
      "   [0.02251343]\n",
      "   [0.02599359]\n",
      "   ...\n",
      "   [0.00205751]\n",
      "   [0.00184763]\n",
      "   [0.00168889]]\n",
      "\n",
      "  [[0.01881251]\n",
      "   [0.02016378]\n",
      "   [0.02245561]\n",
      "   ...\n",
      "   [0.00228231]\n",
      "   [0.0022035 ]\n",
      "   [0.00210652]]\n",
      "\n",
      "  [[0.01631386]\n",
      "   [0.01755812]\n",
      "   [0.01905228]\n",
      "   ...\n",
      "   [0.00261915]\n",
      "   [0.00270178]\n",
      "   [0.00266685]]]], shape=(5000, 64, 64, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "def retrieve_data(data_size):\n",
    "    frames = []\n",
    "\n",
    "    for d in trange(data_size):\n",
    "        video = dataset.update().resolve() # Get a new sequence of frames\n",
    "        for frame in video:\n",
    "            frames.append(frame)\n",
    "\n",
    "    return tf.stack(frames)\n",
    "\n",
    "data_size = 500\n",
    "\n",
    "data = retrieve_data(data_size)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.00092624]\n",
      "  [0.00093991]\n",
      "  [0.00097084]\n",
      "  ...\n",
      "  [0.00117568]\n",
      "  [0.00122628]\n",
      "  [0.00127562]]\n",
      "\n",
      " [[0.00095456]\n",
      "  [0.00096992]\n",
      "  [0.00100557]\n",
      "  ...\n",
      "  [0.00109817]\n",
      "  [0.00113969]\n",
      "  [0.0011775 ]]\n",
      "\n",
      " [[0.00101884]\n",
      "  [0.00100347]\n",
      "  [0.00099622]\n",
      "  ...\n",
      "  [0.00100818]\n",
      "  [0.00101746]\n",
      "  [0.00101997]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.00750577]\n",
      "  [0.00688855]\n",
      "  [0.00644731]\n",
      "  ...\n",
      "  [0.00202924]\n",
      "  [0.00205514]\n",
      "  [0.002087  ]]\n",
      "\n",
      " [[0.00619241]\n",
      "  [0.00554471]\n",
      "  [0.00504339]\n",
      "  ...\n",
      "  [0.00208191]\n",
      "  [0.00210063]\n",
      "  [0.00212973]]\n",
      "\n",
      " [[0.00538733]\n",
      "  [0.00473134]\n",
      "  [0.00417268]\n",
      "  ...\n",
      "  [0.00210815]\n",
      "  [0.00212434]\n",
      "  [0.00216598]]], shape=(64, 64, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 48)        1776      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 48)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 2, 2, 16)          12304     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 16)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,080\n",
      "Trainable params: 14,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 4, 4, 4)           0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 4)        260       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 64, 64, 64)       16448     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 64, 64, 1)         65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,773\n",
      "Trainable params: 16,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Conv2D=keras.layers.Conv2D\n",
    "MaxPool2D=keras.layers.MaxPooling2D\n",
    "Dense=keras.layers.Dense\n",
    "Flatten=keras.layers.Flatten\n",
    "Reshape=keras.layers.Reshape\n",
    "Input = keras.layers.Input\n",
    "Sequential = keras.Sequential\n",
    "Conv2DTranspose = keras.layers.Conv2DTranspose\n",
    "\n",
    "bottleneck_size = 64\n",
    "n_filters = 4\n",
    "\n",
    "class AutoEncoder(Model):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Sequential([\n",
    "            Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 1)),\n",
    "            Conv2D(48, (6, 6), activation='relu', padding='same', strides=4),\n",
    "            MaxPool2D((2, 2), padding='same'),\n",
    "            Conv2D(16, (4, 4), activation='relu', padding='same', strides=4),\n",
    "            MaxPool2D((2, 2), padding='same'),\n",
    "            Flatten()\n",
    "            ])\n",
    "        \n",
    "        self.decoder = Sequential([\n",
    "            Input(shape=(bottleneck_size,)),\n",
    "            Reshape(target_shape=(4, 4, n_filters)),\n",
    "            Conv2DTranspose(n_filters, (4, 4), strides=(4, 4), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(64, (8, 8), strides=(4, 4), activation='relu', padding='same'),\n",
    "            Conv2D(1, (1, 1), activation='linear', padding='same')\n",
    "            ])\n",
    "    def call(self, input):\n",
    "        x = self.encoder(input)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.encoder.build(input_shape=(None, 64, 64, 1))\n",
    "autoencoder.encoder.summary()\n",
    "\n",
    "autoencoder.decoder.build(input_shape=(None, bottleneck_size))\n",
    "autoencoder.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hwc_venv",
   "language": "python",
   "name": "hwc_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
